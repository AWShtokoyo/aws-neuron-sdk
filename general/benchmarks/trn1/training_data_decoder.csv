Model,Instance-Type,Training Data-Type,Nodes,Topology,Microbatch,Global Minibatch, Optimizer, Sequence Length, Performance [seq/sec],Strong/Weak Scaling,Neuron Version,Neuron Tutorial/Example,Pytorch Neuron(torch-neuronx) Version, OS Type.
GPT3-23B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,32,TP=8 DP=32 PP=4,1,1024,AdamW,2048,102,strong scaling,2.16.0,`Nemo Megatron <https://github.com/aws-neuron/aws-neuron-parallelcluster-samples/blob/master/examples/jobs/neuronx-nemo-megatron-gpt-job.md>`_,1.13.1.1.13.0, U20
GPT3-46B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,32,TP=8 DP=16 PP=8,1,1024,AdamW,2048,47,strong scaling,2.16.0,`Nemo Megatron <https://github.com/aws-neuron/aws-neuron-parallelcluster-samples/blob/master/examples/jobs/neuronx-nemo-megatron-gpt-job.md>`_,1.13.1.1.13.0, U20
GPT3-175B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,32,TP=32 DP=4 PP=8,1,1024,AdamW,2048,13.3,strong scaling,2.15.2,`Nemo Megatron <https://github.com/aws-neuron/aws-neuron-parallelcluster-samples/blob/master/examples/jobs/neuronx-nemo-megatron-gpt-job.md>`_,1.13.1.1.12.0, U20
Llama2-7B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,32,TP=8 DP=4 PP=4,1,1024,AdamW,4096,88,strong scaling,2.16.0,`Nemo Megatron <https://github.com/aws-neuron/aws-neuron-parallelcluster-samples/blob/master/examples/jobs/neuronx-nemo-megatron-llamav2-job.md>`_,1.13.1.1.13.0, U20
Llama2-13B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,16,TP=8 DP=4 PP=4,1,1024,AdamW,4096,64,strong scaling,2.16.0,`Nemo Megatron <https://github.com/aws-neuron/aws-neuron-parallelcluster-samples/blob/master/examples/jobs/neuronx-nemo-megatron-llamav2-job.md>`_,1.13.1.1.13.0, U20
Llama2-7B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+FP32Optimizer,16,TP=8 DP=64,1,1024,AdamW,4096,93.3,strong scaling,2.16.0,`NeuronX Distributed <https://github.com/aws-neuron/aws-neuron-sdk/blob/master/libraries/neuronx-distributed/tutorials/training_llama2_7b.rst>`_,1.13.1.1.13.0, U20
Llama2-13B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+FP32Optimizer,32,TP=8 DP=32 PP=4,1,1024,AdamW,4096,85.65,strong scaling,2.16.0,`NeuronX Distributed <https://github.com/aws-neuron/aws-neuron-sdk/blob/master/libraries/neuronx-distributed/tutorials/training_llama2_tp_pp.rst>`_,1.13.1.1.13.0, U20
Llama2-70B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+FP32Optimizer,32,TP=32 DP=8 PP=4,1,1024,AdamW,4096,15.34,strong scaling,2.16.0,`NeuronX Distributed <https://github.com/aws-neuron/aws-neuron-sdk/blob/master/libraries/neuronx-distributed/tutorials/training_llama2_tp_pp.rst>`_,1.13.1.1.13.0, U20
