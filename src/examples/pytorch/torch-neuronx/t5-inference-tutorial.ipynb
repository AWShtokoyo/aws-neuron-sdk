{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Decoding with T5 models on Trn1 or Inf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial we will compile and deploy a pretrained T5 model for accelerated inference on Neuron. \n",
    "\n",
    "This tutorial will use the [t5-large](https://huggingface.co/t5-large) model. The T5 model can be used for machine translation, document summarization, question answering, and classification tasks. \n",
    "\n",
    "This tutorial has the following main sections:\n",
    "\n",
    "1. Install dependencies\n",
    "1. Compile the T5 model\n",
    "1. Run inference with greedy decoding on Neuron\n",
    "\n",
    "This Jupyter notebook should be run on a Trn1 instance (`trn1.2xlarge` or larger.) or Inf2 instance (`inf2.xlarge` or larger.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "The code in this tutorial is written for Jupyter Notebooks. To use Jupyter Notebook on the Neuron instance, you\n",
    "can use this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `transformers`\n",
    "- `optimum-neuron`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the Trn1/Inf2 setup guide. The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers==4.31.0 optimum-neuron==0.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ðŸ¤— Optimum Neuron is the interface between the ðŸ¤— Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks. In this tutorial we use ðŸ¤— HuggingFace Optimum Neuron's generate() method instead of ðŸ¤— [transformers's generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) to perform greedy decoding. Optimum Neuron takes care of padding the inputs which is necessary to infer on Neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model into an AWS Neuron optimized TorchScript\n",
    "\n",
    "In the following section, we load the T5 model, compile the model's encoder and decoder for Neuron using `torch_neuronx.trace()`, and save the optimized encoder and decoder as `TorchScript`. \n",
    "\n",
    "`torch_neuronx` can only trace functions with positional arguments. The T5 encoder and decoder both use keyword arguments. To trace them, we write wrappers that convert keyword arguments to positional arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.t5.modeling_t5 import T5Stack\n",
    "\n",
    "class EncoderWrapper(torch.nn.Module):\n",
    "    '''\n",
    "        We will trace an instance of the EncoderWrapper. \n",
    "        This wrapper just converts positional args to kwargs. \n",
    "    '''\n",
    "\n",
    "    def __init__(self, encoder: T5Stack):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        # This is the core functionality we want to trace. \n",
    "        return self.encoder(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            output_attentions=False,\n",
    "                            output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the decoder wrapper, in addition to converting keyword arguments to positional arguments we add support for attention caching. Generating text from the encoder decoder models is an autoregressive process. For each invocation, we have to compute the key and value states of the attention heads repeatedly. To improve the performance, we cache the key and value states. This cache is what HuggingFace transformers code refers to as `past_key_values`.\n",
    "\n",
    "In HuggingFace transformers, the `past_key_values` are updated outside the decoder. This works for training and evaluation but for inference we want to perform them within a single trace. This way, we can optimize across both the decoder execution and cache update. So, we move the cache update within the decoder wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "\n",
    "class DecoderWrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 decoder: T5Stack, \n",
    "                 lm_head: torch.nn.Linear,\n",
    "                 model_config,\n",
    "                 num_beams: int, \n",
    "                 max_length: int,\n",
    "                 device: str):\n",
    "                 \n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.lm_head = lm_head\n",
    "        self.model_dim=model_config.d_model\n",
    "        self.device = device\n",
    "        self.num_beams = num_beams\n",
    "\n",
    "        num_heads=model_config.num_heads\n",
    "        num_decoder_layers=model_config.num_decoder_layers\n",
    "\n",
    "        # Iniitialize the cache\n",
    "        shape = (num_beams,num_heads,max_length,model_config.d_kv)\n",
    "        if device == \"cpu\":\n",
    "            cache = []\n",
    "            for _ in range(num_decoder_layers * 4):\n",
    "                cache.append(torch.ones(shape, dtype=torch.float32))\n",
    "            self.past_key_values = cache\n",
    "        elif device == \"xla\":\n",
    "            cache = []\n",
    "            for _ in range(num_decoder_layers * 4):\n",
    "                cache.append(Parameter(torch.ones(shape, dtype=torch.float32), \n",
    "                                      requires_grad=False))    \n",
    "            self.past_key_values = torch.nn.ParameterList(cache)\n",
    "\n",
    "    # We add the states for the newly generated token to the cache\n",
    "    def update_past(self, past_key_values):\n",
    "        new_past = []\n",
    "        for past_layer in past_key_values:\n",
    "            new_past_layer = list(past_layer)\n",
    "            # We just need to update the self attention cache\n",
    "            for i in range(len(new_past_layer[:2])):\n",
    "                new_past_layer[i] = past_layer[i][:, :, 1:]\n",
    "            new_past += [new_past_layer,]\n",
    "        return new_past\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                decoder_attention_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                beam_idx,\n",
    "                **kwargs):\n",
    "\n",
    "        past_key_values = self.past_key_values\n",
    "\n",
    "        # The cache is stored in a flatten form. \n",
    "        # We order the cache per layer before passing it to the decoder. \n",
    "        # Each layer has 4 tensors, so we group by 4. \n",
    "        past_key_values = [past_key_values[i*4:i*4+4] for i in range(0, int(len(past_key_values)/4))]\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False)\n",
    "\n",
    "        last_hidden_state = decoder_output['last_hidden_state']\n",
    "        past_key_values = decoder_output['past_key_values']\n",
    "\n",
    "        last_hidden_state = last_hidden_state * (self.model_dim**-0.5)\n",
    "        lm_logits = self.lm_head(last_hidden_state)\n",
    "\n",
    "        past_key_values = self.update_past(past_key_values)\n",
    "\n",
    "        # We flatten the cache to a single array. \n",
    "        # This is required for the input output aliasing to work\n",
    "        past_key_values = [vec for kv_per_layer in past_key_values for vec in kv_per_layer]\n",
    "\n",
    "        if self.device == \"cpu\":\n",
    "            self.past_key_values = past_key_values\n",
    "\n",
    "        return [lm_logits] + past_key_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The key value cache that the decoder uses has both the self attention and cross attention states. While the self attention states are updated with each decoder call, the cross attention states remain unchanged. So, the cross attention states have to be computed just once. Transformer's t5 model initiializes the cross attention cache state on the first decoder invocation, but this is harder to do on Neuron. Similar to `torch.jit.trace()` the `torch_neuronx.trace()` produces a function that has a fixed control flow, i.e. there are no conditional executions. So we cannot choose to conditionally initialize the cache in the first iteration. Instead, we can compute the initial cache state outside the generation flow and pass the cache to it. To do so, we create a cache initalizer. This function will be run once before each generation to get the cache state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.t5.modeling_t5 import T5LayerCrossAttention\n",
    "\n",
    "class CacheInitializer(torch.nn.Module):\n",
    "    '''\n",
    "        Cache initializer is used once per input to compute the \n",
    "        cross attention key and value states. \n",
    "    '''\n",
    "\n",
    "    def __init__(self, decoder, model_config, batch_size, max_length, device):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.model_config = model_config\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, encoder_hidden_states):\n",
    "        decoder_blocks = self.decoder.block\n",
    "\n",
    "        present_key_value_states = []\n",
    "        for block in decoder_blocks:\n",
    "\n",
    "            # Self attention kv states are initialized to zeros.\n",
    "            self_attn_kv_state = list(\n",
    "                torch.zeros((self.batch_size, self.model_config.num_heads, \n",
    "                             self.max_length, self.model_config.d_kv), \n",
    "                            dtype=torch.float32, \n",
    "                            device=self.device) for i in range(0, 2))\n",
    "\n",
    "            # Cross attention has to be initialized with the encoder hidden state\n",
    "            cross_attention: T5LayerCrossAttention = block.layer[1]\n",
    "            attention = cross_attention.EncDecAttention\n",
    "\n",
    "            def shape(states):\n",
    "                return states.view(self.batch_size, -1, attention.n_heads, \n",
    "                                   attention.key_value_proj_dim).transpose(1, 2)\n",
    "\n",
    "            key_states = shape(attention.k(encoder_hidden_states))\n",
    "            value_states = shape(attention.v(encoder_hidden_states))\n",
    "            cross_attn_kv_state = [key_states, value_states]\n",
    "\n",
    "            # We add concatenate the self and cross attentions to create \n",
    "            # the KV cache for the decoder block\n",
    "            present_key_value_state = self_attn_kv_state + cross_attn_kv_state\n",
    "            present_key_value_states = present_key_value_states + present_key_value_state\n",
    "\n",
    "        return present_key_value_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a T5 model wrapper to make it compatible with our traced encoder and decoder. \n",
    "\n",
    "There are two reasons for having this wrapper, \n",
    "\n",
    "1. The encoder and decoder traces can only be invoked with positional arguments. But the HuggingFace transformers code is written with keyword arguments. So we override the functions that invoke encoder and decoder to call with positional arguments. \n",
    "1. The generate() function in the NeuronGenerationMixin performs cache update within the CPU. As we are handling the cache within the DecoderWrapper, we disable the cache update on CPU. \n",
    "\n",
    "Let's also override the `generate()` function so that it will intialize the cache using the cache initalizer before starting the greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from transformers.generation.utils import ModelOutput\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "from optimum.neuron.generation import NeuronGenerationMixin\n",
    "\n",
    "class T5Wrapper(T5ForConditionalGeneration, NeuronGenerationMixin):\n",
    "\n",
    "    def _prepare_encoder_decoder_kwargs_for_generation(\n",
    "        self, \n",
    "        inputs_tensor: torch.Tensor, \n",
    "        model_kwargs, \n",
    "        model_input_name: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        encoder = self.get_encoder()\n",
    "        model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(inputs_tensor, \n",
    "                                                               model_kwargs[\"attention_mask\"])\n",
    "        return model_kwargs\n",
    "\n",
    "    # Override to cut the input_ids to just last token\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        decoder_attention_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # cut decoder_input_ids as past is cached\n",
    "        input_ids = input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"decoder_input_ids\": input_ids,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"decoder_attention_mask\": decoder_attention_mask,\n",
    "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "    \n",
    "    '''\n",
    "        We update the cache in the decoder trace, \n",
    "        so let's disable the update in NeuronGenerationMixin\n",
    "    '''\n",
    "    def _update_model_kwargs_for_xla_generation(\n",
    "        self,\n",
    "        outputs: ModelOutput,\n",
    "        model_kwargs: Dict[str, Any],\n",
    "        batch_size: int,\n",
    "        is_encoder_decoder: bool = False,\n",
    "        standardize_cache_format: bool = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        seq_length: Optional[int] = None,\n",
    "        use_cache: bool = True,\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        def _update_attention(model_kwargs, is_encoder_decoder):\n",
    "            \"\"\"Updates the appropriate attention mask -- encoder-decoder models use `decoder_attention_mask`\"\"\"\n",
    "\n",
    "            attention_mask_name = \"decoder_attention_mask\" if is_encoder_decoder else \"attention_mask\"\n",
    "            attention_mask = model_kwargs.pop(attention_mask_name)\n",
    "            attention_mask_update_slice = torch.ones(\n",
    "                (batch_size, 1), \n",
    "                dtype=attention_mask.dtype, \n",
    "                device=attention_mask.device\n",
    "            )\n",
    "            attention_mask = torch.cat([attention_mask[:, 1:], \n",
    "                                        attention_mask_update_slice], dim=-1)\n",
    "            mask = {attention_mask_name: attention_mask}\n",
    "            return mask\n",
    "\n",
    "        mask = _update_attention(model_kwargs, is_encoder_decoder)\n",
    "        # sets the updated variables (mask and past_key_values)\n",
    "        model_kwargs.update(mask)\n",
    "\n",
    "        # Set a mock cache tensor for NeuronGenerationMixin\n",
    "        model_kwargs[\"past_key_values\"] = torch.tensor([])\n",
    "\n",
    "        return model_kwargs\n",
    "\n",
    "    def generate(self,\n",
    "                 cache_initializer,\n",
    "                 tokenizer: T5Tokenizer,\n",
    "                 prompt: str,\n",
    "                 max_length: int,\n",
    "                 num_beams: int,\n",
    "                 num_return_sequences: int,\n",
    "                 device: str):\n",
    "\n",
    "        batch_encoding = tokenizer(prompt, \n",
    "                                   max_length=max_length, \n",
    "                                   truncation=True, \n",
    "                                   padding='max_length',\n",
    "                                   return_tensors=\"pt\")\n",
    "\n",
    "        encoder_output = self.encoder(batch_encoding['input_ids'],\n",
    "                                      batch_encoding['attention_mask'])\n",
    "        last_hidden_state = encoder_output[\"last_hidden_state\"]\n",
    "        \n",
    "        encoder_hidden_states = torch.concat(\n",
    "            [tensor.unsqueeze(0).repeat(num_beams, 1, 1) for tensor in last_hidden_state])\n",
    "\n",
    "        # Initialize the cache and mask\n",
    "        past_key_values = cache_initializer(encoder_hidden_states)\n",
    "        decoder_attention_mask = torch.cat([torch.zeros((1, max_length), dtype=torch.int32),\n",
    "                                            torch.ones((1, 1), dtype=torch.int32)], axis=1)\n",
    "\n",
    "        # copy the new cache state to the decoder\n",
    "        if device == \"xla\":\n",
    "            for state, tensor in zip(self.decoder.parameters(), past_key_values):\n",
    "                state.copy_(tensor)\n",
    "        else:\n",
    "            self.decoder.past_key_values = past_key_values\n",
    "        \n",
    "        output = super().generate(**batch_encoding,\n",
    "                                  max_length=max_length,\n",
    "                                  num_beams=num_beams,\n",
    "                                  num_return_sequences=num_return_sequences,\n",
    "                                  do_sample=False,\n",
    "                                  use_cache=True,\n",
    "                                  decoder_attention_mask=decoder_attention_mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        **kwargs\n",
    "    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n",
    "\n",
    "        hidden_states = encoder_outputs[\"last_hidden_state\"]\n",
    "\n",
    "        if not hasattr(self, 'beam_idx'):\n",
    "            # Infering the number of beams from the attention mask\n",
    "            num_beams = attention_mask.shape[0]\n",
    "            self.beam_idx = torch.arange(0, num_beams, dtype=torch.int64)\n",
    "\n",
    "        decoder_outputs = self.decoder(\n",
    "            decoder_input_ids,\n",
    "            decoder_attention_mask,\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            self.beam_idx\n",
    "        )\n",
    "\n",
    "        lm_logits = decoder_outputs[0]\n",
    "\n",
    "        return Seq2SeqLMOutput(logits=lm_logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test inference on CPU with all the wrappers before tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some run parameters\n",
    "\n",
    "model_name = \"t5-large\"\n",
    "num_beams = 1\n",
    "num_return_sequences = 1\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "1 Lassen Sie uns gutes Essen essen.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "\n",
    "prompt=\"translate English to German: Lets eat good food.\"\n",
    "        \n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, model_max_length=max_length)\n",
    "model = T5Wrapper.from_pretrained(model_name)\n",
    "\n",
    "cache_initializer = CacheInitializer(model.decoder, model.config, num_beams, max_length, \"cpu\")\n",
    "model.encoder = EncoderWrapper(model.encoder)\n",
    "setattr(model.encoder, 'main_input_name', 'input_ids')  # Attribute required by beam search\n",
    "\n",
    "model.decoder = DecoderWrapper(decoder=model.decoder,\n",
    "                                lm_head=model.lm_head,\n",
    "                                model_config=model.config,\n",
    "                                num_beams=num_beams,\n",
    "                                max_length=max_length,\n",
    "                                device=\"cpu\")\n",
    "\n",
    "output = model.generate(tokenizer=tokenizer,\n",
    "                        cache_initializer=cache_initializer,\n",
    "                        prompt=prompt,\n",
    "                        max_length=max_length,\n",
    "                        num_beams=num_beams,\n",
    "                        num_return_sequences=num_return_sequences,\n",
    "                        device=\"cpu\")\n",
    "\n",
    "results = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "\n",
    "print('Results:')\n",
    "for i, summary in enumerate(results):\n",
    "    print(i + 1, summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the wrappers are running as expected, let's trace the encoder, decoder and the cache initializer. To trace these functions, we pass the function and a sample input to the trace function. The result of the trace stage will be a static executable where the operations to be run upon inference are determined during compilation. This means that when inferring, the resulting Neuron model must be executed with tensors that are the exact same shape as those provided at compilation time. If a model is given a tensor at inference time whose shape does not match the tensor given at compilation time, an error will occur.\n",
    "\n",
    "The decoder wrapper returns the new state of the cache as an output which is copied back to the CPU. As the cache is a large tensor, copying it to and from the XLA device for each decoder invocation will significantly slow down the inference. Instead, we can use input output aliasing, a feature of `torch_neuronx` to keep these tensors on device rather than copying back to the CPU. To use input output aliasing, we need to map the outputs to input parameters while tracing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "def trace_encoder(model: T5ForConditionalGeneration,\n",
    "                  tokenizer: T5Tokenizer,\n",
    "                  max_length: int):\n",
    "\n",
    "    # Trace encoder\n",
    "    batch_encoding = tokenizer(\"translate English to German: Lets go home now\",\n",
    "                               max_length=max_length,\n",
    "                               truncation=True,\n",
    "                               padding='max_length', \n",
    "                               return_tensors=\"pt\")\n",
    "    input_ids = batch_encoding['input_ids']\n",
    "    attention_mask = batch_encoding['attention_mask']\n",
    "\n",
    "    encoder = EncoderWrapper(model.encoder)\n",
    "    traced_encoder = torch_neuronx.trace(encoder, \n",
    "                                         (input_ids, attention_mask), \n",
    "                                         compiler_workdir=\"/tmp/encoder/\")\n",
    "    # Attribute required by beam search\n",
    "    setattr(traced_encoder, 'main_input_name', 'input_ids')  \n",
    "\n",
    "    return traced_encoder\n",
    "\n",
    "\n",
    "def trace_decoder(model: T5ForConditionalGeneration,\n",
    "                  num_beams: int,\n",
    "                  max_length: int):\n",
    "\n",
    "    decoder = DecoderWrapper(decoder=model.decoder,\n",
    "                             lm_head=model.lm_head,\n",
    "                             model_config=model.config,\n",
    "                             num_beams=num_beams,\n",
    "                             max_length=max_length,\n",
    "                             device=\"xla\")\n",
    "\n",
    "    # We create mock inputs so we can trace the decoder\n",
    "    decoder_input_ids = torch.ones((num_beams, 1), dtype=torch.int64)\n",
    "    decoder_attention_mask = torch.ones((num_beams, max_length + 1), dtype=torch.int32)\n",
    "    encoder_attention_mask = torch.ones((num_beams, max_length), dtype=torch.int64)\n",
    "    encoder_hidden_states = torch.ones((num_beams, max_length, model.config.d_model), dtype=torch.float32)\n",
    "\n",
    "    beam_idx = torch.arange(0, num_beams, dtype=torch.int64)\n",
    "\n",
    "    traced_decoder = torch_neuronx.trace(decoder, (\n",
    "        decoder_input_ids,\n",
    "        decoder_attention_mask,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask,\n",
    "        beam_idx\n",
    "    ), \n",
    "    input_output_aliases={decoder.past_key_values[i]:i+1 for i in range(len(decoder.past_key_values))}, \n",
    "    compiler_workdir=\"/tmp/decoder/\")\n",
    "\n",
    "    return traced_decoder\n",
    "\n",
    "def trace_cache_initializer(model: T5ForConditionalGeneration,\n",
    "                            num_beams: int,\n",
    "                            max_length: int):\n",
    "        \n",
    "    encoder_hidden_states = torch.ones((num_beams, \n",
    "                                        max_length, \n",
    "                                        model.config.d_model), dtype=torch.float32)\n",
    "\n",
    "    cache_initializer = CacheInitializer(model.decoder,\n",
    "                                         model.config, \n",
    "                                         num_beams, \n",
    "                                         max_length, \n",
    "                                         \"xla\")\n",
    "    cache_initializer = torch_neuronx.trace(cache_initializer, \n",
    "                                            encoder_hidden_states, \n",
    "                                            compiler_workdir=\"/tmp/cache_init/\")\n",
    "\n",
    "    return cache_initializer\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, model_max_length=max_length)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# We enable this flag to ensure model uses attention key value caching\n",
    "model.config.use_cache = True\n",
    "\n",
    "traced_encoder = trace_encoder(model, tokenizer, max_length)\n",
    "traced_decoder = trace_decoder(model, num_beams, max_length)\n",
    "traced_cache_initializer = trace_cache_initializer(model, num_beams, max_length)\n",
    "\n",
    "torch.jit.save(traced_encoder, \"TracedEncoder.pt\")\n",
    "torch.jit.save(traced_decoder, \"TracedDecoder.pt\")\n",
    "torch.jit.save(traced_cache_initializer, \"TracedCacheInitializer.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference with greedy decoding\n",
    "Now that we have the traced model, let's use it for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLA Texts:\n",
      "1 Lassen Sie uns gutes Essen essen.\n"
     ]
    }
   ],
   "source": [
    "runtime = torch.classes.neuron.Runtime()\n",
    "runtime.initialize()\n",
    "runtime.set_default_neuron_cores(0, 1)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5Wrapper.from_pretrained(model_name)\n",
    "\n",
    "model.encoder = torch.jit.load(\"TracedEncoder.pt\")\n",
    "# Attribute required by beam search\n",
    "setattr(model.encoder, 'main_input_name', 'input_ids')  \n",
    "\n",
    "model.decoder = torch.jit.load(\"TracedDecoder.pt\")\n",
    "torch_neuronx.move_trace_to_device(model.decoder, 0)\n",
    "\n",
    "cache_init_trace = torch.jit.load(\"TracedCacheInitializer.pt\")\n",
    "\n",
    "\n",
    "output = model.generate(tokenizer=tokenizer,\n",
    "                        cache_initializer=cache_init_trace,\n",
    "                        prompt=\"translate English to German: Lets eat good food.\",\n",
    "                        max_length=max_length,\n",
    "                        num_beams=num_beams,\n",
    "                        num_return_sequences=num_return_sequences,\n",
    "                        device=\"xla\")\n",
    "\n",
    "results = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "\n",
    "print('XLA Texts:')\n",
    "for i, summary in enumerate(results):\n",
    "    print(i + 1, summary)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
