{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and Deploying HuggingFace Pretrained BERT\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "\n",
    "Before running the following verify this Jupyter notebook is running “conda_aws_neuron_pytorch_p36” kernel. You can select the Kernel from the “Kernel -> Change Kernel” option on the top of this Jupyter notebook page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model into an AWS Neuron optimized TorchScript\n",
    "\n",
    "This step can be done by calling `torch.neuron.trace` method on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e05fd423b484afca19686cd2dd9492c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd9e9d43802499ca73032fce78d396d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3f75fa35454155aa29c5efd9eedbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:There are 3 ops of 1 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, (For more information see https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/neuron-cc-ops/neuron-cc-ops-pytorch.md)\n",
      "INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 714, fused = 694, percent fused = 97.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/ops/aten.py:1547: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/ops/aten.py:1547: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:Neuron:Compiling function _NeuronGraph$661 with neuron-cc\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/bin/neuron-cc compile /tmp/tmpb4ha43x7/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /tmp/tmpb4ha43x7/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Add_136:0\"]} --verbose 35'\n",
      "INFO:Neuron:Successfully embedded GraphDef into MetaNeff for _NeuronGraph#62\n",
      "/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch/jit/_trace.py:441: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:73.)\n",
      "  outs = wrap_retval(mod(*_clone_inputs(inputs)))\n",
      "/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/graph.py:525: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:73.)\n",
      "  return self.func(*inputs)\n",
      "INFO:Neuron:Number of arithmetic operators (post-compilation) before = 714, compiled = 694, percent compiled = 97.2%\n",
      "INFO:Neuron:The neuron partitioner created 1 sub-graphs\n",
      "INFO:Neuron:Neuron successfully compiled 1 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 100.0%\n",
      "INFO:Neuron:Compiled these operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 96\n",
      "INFO:Neuron: => aten::add: 108\n",
      "INFO:Neuron: => aten::addmm: 2\n",
      "INFO:Neuron: => aten::contiguous: 12\n",
      "INFO:Neuron: => aten::div: 12\n",
      "INFO:Neuron: => aten::dropout: 38\n",
      "INFO:Neuron: => aten::gelu: 12\n",
      "INFO:Neuron: => aten::layer_norm: 25\n",
      "INFO:Neuron: => aten::matmul: 96\n",
      "INFO:Neuron: => aten::permute: 48\n",
      "INFO:Neuron: => aten::select: 1\n",
      "INFO:Neuron: => aten::size: 96\n",
      "INFO:Neuron: => aten::slice: 1\n",
      "INFO:Neuron: => aten::softmax: 12\n",
      "INFO:Neuron: => aten::t: 74\n",
      "INFO:Neuron: => aten::tanh: 1\n",
      "INFO:Neuron: => aten::transpose: 12\n",
      "INFO:Neuron: => aten::view: 48\n",
      "INFO:Neuron:Not compiled operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 2 [supported]\n",
      "INFO:Neuron: => aten::ScalarImplicit: 1 [supported]\n",
      "INFO:Neuron: => aten::add: 2 [supported]\n",
      "INFO:Neuron: => aten::arange: 1 [supported]\n",
      "INFO:Neuron: => aten::embedding: 3 [not supported]\n",
      "INFO:Neuron: => aten::expand: 1 [supported]\n",
      "INFO:Neuron: => aten::mul: 1 [supported]\n",
      "INFO:Neuron: => aten::rsub: 1 [supported]\n",
      "INFO:Neuron: => aten::size: 2 [supported]\n",
      "INFO:Neuron: => aten::slice: 2 [supported]\n",
      "INFO:Neuron: => aten::to: 1 [supported]\n",
      "INFO:Neuron: => aten::unsqueeze: 3 [supported]\n",
      "/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch/jit/_trace.py:728: UserWarning: The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.\n",
      "  \"The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.\"\n",
      "/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/convert.py:450: RuntimeWarning: Unexpected end-group tag: Not all data was converted\n",
      "  metaneff.ParseFromString(metaneff_bytes)\n"
     ]
    }
   ],
   "source": [
    "# You may save the content of this cell as compile_bert.py and run it with python3.\n",
    "import tensorflow  # to workaround a protobuf version conflict issue\n",
    "import torch\n",
    "import torch.neuron\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import transformers\n",
    "\n",
    "# Build tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "model = None\n",
    "\n",
    "if transformers.__version__.startswith(\"4.\"):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\", return_dict=False)\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "# Setup some example inputs\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "max_length=128\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Run the original PyTorch model on compilation exaple\n",
    "paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "\n",
    "# Convert example inputs to a format that is compatible with TorchScript tracing\n",
    "example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask'], paraphrase['token_type_ids']\n",
    "example_inputs_not_paraphrase = not_paraphrase['input_ids'], not_paraphrase['attention_mask'], not_paraphrase['token_type_ids']\n",
    "\n",
    "# Run torch.neuron.trace to generate a TorchScript that is optimized by AWS Neuron\n",
    "model_neuron = torch.neuron.trace(model, example_inputs_paraphrase)\n",
    "\n",
    "# Verify the TorchScript works on both example inputs\n",
    "paraphrase_classification_logits_neuron = model_neuron(*example_inputs_paraphrase)\n",
    "not_paraphrase_classification_logits_neuron = model_neuron(*example_inputs_not_paraphrase)\n",
    "\n",
    "# Save the TorchScript for later use\n",
    "model_neuron.save('bert_neuron.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example uses BERT-base. A full list of HuggingFace's pretrained BERT models can be found in the BERT section on this page https://huggingface.co/transformers/pretrained_models.html.\n",
    "\n",
    "You may inspect `model_neuron.graph` to see which part is running on CPU versus running on the accelerator. All native `aten` operators in the graph will be running on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self : __torch__.torch_neuron.convert.AwsNeuronGraphModule,\n",
      "      %tensor.4 : Long(1:128, 128:1, requires_grad=0, device=cpu),\n",
      "      %tensor.1 : Long(1:128, 128:1, requires_grad=0, device=cpu),\n",
      "      %4 : Long(1:128, 128:1, requires_grad=0, device=cpu)):\n",
      "  %11 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %12 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %13 : int = prim::Constant[value=9223372036854775807]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %14 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %15 : Long(1:128, 128:1, requires_grad=0, device=cpu) = aten::slice(%tensor.1, %11, %12, %13, %14) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %16 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %17 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %18 : int = prim::Constant[value=9223372036854775807]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %19 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %20 : Long(1:128, 128:1, requires_grad=0, device=cpu) = aten::slice(%15, %16, %17, %18, %19) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %21 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %22 : Long(1:128, 1:128, 128:1, requires_grad=0, device=cpu) = aten::unsqueeze(%20, %21) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %23 : int = prim::Constant[value=2]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %tensor.2 : Long(1:128, 1:128, 1:128, 128:1, requires_grad=0, device=cpu) = aten::unsqueeze(%22, %23) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %37 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %38 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %39 : int = prim::Constant[value=9223372036854775807]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %40 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %41 : Long(1:128, 1:128, 1:128, 128:1, requires_grad=0, device=cpu) = aten::slice(%tensor.2, %37, %38, %39, %40) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %42 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %43 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %44 : int = prim::Constant[value=9223372036854775807]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %45 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %46 : Long(1:128, 1:128, 1:128, 128:1, requires_grad=0, device=cpu) = aten::slice(%41, %42, %43, %44, %45) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %47 : int = prim::Constant[value=2]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %48 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %49 : int = prim::Constant[value=9223372036854775807]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %50 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %51 : Long(1:128, 1:128, 1:128, 128:1, requires_grad=0, device=cpu) = aten::slice(%46, %47, %48, %49, %50) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %52 : int = prim::Constant[value=3]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %53 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %54 : int = prim::Constant[value=9223372036854775807]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %55 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %tensor.3 : Long(1:128, 1:128, 1:128, 128:1, requires_grad=0, device=cpu) = aten::slice(%51, %52, %53, %54, %55) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:34:0\n",
      "  %57 : int = prim::Constant[value=6]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:76:0\n",
      "  %58 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:76:0\n",
      "  %59 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:76:0\n",
      "  %60 : None = prim::Constant()\n",
      "  %61 : Float(1:128, 1:128, 1:128, 128:1, requires_grad=0, device=cpu) = aten::to(%tensor.3, %57, %58, %59, %60) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:76:0\n",
      "  %62 : float = prim::Constant[value=1.]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %63 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %64 : Float(1:128, 1:128, 1:128, 128:1, requires_grad=0, device=cpu) = aten::rsub(%61, %62, %63) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %65 : Double(requires_grad=0, device=cpu) = prim::Constant[value={-10000}]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %66 : Float(1:128, 1:128, 1:128, 128:1, requires_grad=0, device=cpu) = aten::mul(%64, %65) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %67 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %68 : int = aten::size(%tensor.4, %67) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %num.1 : Long(device=cpu) = prim::NumToTensor(%68)\n",
      "  %70 : int = prim::Constant[value=4]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/prim.py:41:0\n",
      "  %71 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/prim.py:41:0\n",
      "  %72 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/prim.py:41:0\n",
      "  %73 : None = prim::Constant()\n",
      "  %tensor.5 : Long(requires_grad=0, device=cpu) = aten::to(%num.1, %70, %71, %72, %73) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/prim.py:41:0\n",
      "  %75 : int = prim::Constant[value=3]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:38:0\n",
      "  %76 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:38:0\n",
      "  %77 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:38:0\n",
      "  %78 : None = prim::Constant()\n",
      "  %79 : Int(requires_grad=0, device=cpu) = aten::to(%tensor.5, %75, %76, %77, %78) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:38:0\n",
      "  %106 : int = aten::Int(%79)\n",
      "  %80 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %81 : int = aten::size(%tensor.4, %80) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %num : Long(device=cpu) = prim::NumToTensor(%81)\n",
      "  %83 : int = prim::Constant[value=4]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/prim.py:41:0\n",
      "  %84 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/prim.py:41:0\n",
      "  %85 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/prim.py:41:0\n",
      "  %86 : None = prim::Constant()\n",
      "  %tensor.6 : Long(requires_grad=0, device=cpu) = aten::to(%num, %83, %84, %85, %86) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/prim.py:41:0\n",
      "  %88 : int = prim::Constant[value=3]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:38:0\n",
      "  %89 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:38:0\n",
      "  %90 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:38:0\n",
      "  %91 : None = prim::Constant()\n",
      "  %92 : Int(requires_grad=0, device=cpu) = aten::to(%tensor.6, %88, %89, %90, %91) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:38:0\n",
      "  %107 : int = aten::Int(%92)\n",
      "  %93 : int = prim::Constant[value=4]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:51:0\n",
      "  %94 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:51:0\n",
      "  %95 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:51:0\n",
      "  %96 : None = prim::Constant()\n",
      "  %97 : Long(requires_grad=0, device=cpu) = aten::to(%tensor.6, %93, %94, %95, %96) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:51:0\n",
      "  %98 : Scalar = aten::ScalarImplicit(%97)\n",
      "  %99 : int = prim::Constant[value=4]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %100 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %101 : Device = prim::Constant[value=\"cpu\"]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %102 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %103 : Long(128:1, requires_grad=0, device=cpu) = aten::arange(%98, %99, %100, %101, %102) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %104 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %tensor : Long(1:128, 128:1, requires_grad=0, device=cpu) = aten::unsqueeze(%103, %104) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %108 : int[] = prim::ListConstruct(%106, %107)\n",
      "  %109 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %110 : Long(1:128, 128:1, requires_grad=0, device=cpu) = aten::expand(%tensor, %108, %109) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %111 : Float(28996:768, 768:1, requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %112 : int = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %113 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %114 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %115 : Float(1:98304, 128:768, 768:1, requires_grad=0, device=cpu) = aten::embedding(%111, %tensor.4, %112, %113, %114) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %116 : Float(512:768, 768:1, requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %117 : int = prim::Constant[value=-1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %118 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %119 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %120 : Float(1:98304, 128:768, 768:1, requires_grad=0, device=cpu) = aten::embedding(%116, %110, %117, %118, %119) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %121 : Float(2:768, 768:1, requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %122 : int = prim::Constant[value=-1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %123 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %124 : bool = prim::Constant[value=0]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %125 : Float(1:98304, 128:768, 768:1, requires_grad=0, device=cpu) = aten::embedding(%121, %4, %122, %123, %124) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %126 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %127 : Float(1:98304, 128:768, 768:1, requires_grad=0, device=cpu) = aten::add(%115, %120, %126) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %128 : int = prim::Constant[value=1]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %129 : Float(1:98304, 128:768, 768:1, requires_grad=0, device=cpu) = aten::add(%127, %125, %128) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/resolve_function.py:55:0\n",
      "  %130 : Function = prim::Constant[name=\"neuron_function\"]()\n",
      "  %134 : Byte(137424462:1, requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/decorators.py:316:0\n",
      "  %135 : Byte(1434225:1, requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/decorators.py:316:0\n",
      "  %136 : Byte(36:1, requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>]() # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/decorators.py:316:0\n",
      "  %137 : Tensor[] = prim::ListConstruct(%129, %66)\n",
      "  %138 : Float(1:2, 2:1, requires_grad=0, device=cpu) = neuron::forward_1(%137, %136, %135, %134) # /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/torch_neuron/decorators.py:316:0\n",
      "  %139 : (Float(1:2, 2:1, requires_grad=0, device=cpu)) = prim::TupleConstruct(%138)\n",
      "  %132 : Tensor = prim::TupleUnpack(%139)\n",
      "  %133 : (Tensor) = prim::TupleConstruct(%132)\n",
      "  return (%133)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_neuron.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to copy your saved TorchScript `bert_neuron.pt` to your `inf1` instance.\n",
    "\n",
    "### Deploy the AWS Neuron optimized TorchScript on an `inf1` instance\n",
    "\n",
    "To deploy the AWS Neuron optimized TorchScript on `inf1` instances, you may choose to load the saved TorchScript from disk and skip the slow compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT says that \"The company HuggingFace is based in New York City\" and \"HuggingFace's headquarters are situated in Manhattan\" are paraphrase\n",
      "BERT says that \"The company HuggingFace is based in New York City\" and \"Apples are especially bad for your health\" are not paraphrase\n"
     ]
    }
   ],
   "source": [
    "# You may save the content of this cell as run_bert.py and run it with python3.\n",
    "import torch\n",
    "import torch.neuron\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "# Setup some example inputs\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, max_length=128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, max_length=128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Convert example inputs to a format that is compatible with TorchScript tracing\n",
    "example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask'], paraphrase['token_type_ids']\n",
    "example_inputs_not_paraphrase = not_paraphrase['input_ids'], not_paraphrase['attention_mask'], not_paraphrase['token_type_ids']\n",
    "\n",
    "# Load TorchScript back\n",
    "model_neuron = torch.jit.load('bert_neuron.pt')\n",
    "\n",
    "# Verify the TorchScript works on both example inputs\n",
    "paraphrase_classification_logits_neuron = model_neuron(*example_inputs_paraphrase)\n",
    "not_paraphrase_classification_logits_neuron = model_neuron(*example_inputs_not_paraphrase)\n",
    "classes = ['not paraphrase', 'paraphrase']\n",
    "paraphrase_prediction = paraphrase_classification_logits_neuron[0][0].argmax().item()\n",
    "not_paraphrase_prediction = not_paraphrase_classification_logits_neuron[0][0].argmax().item()\n",
    "print('BERT says that \"{}\" and \"{}\" are {}'.format(sequence_0, sequence_2, classes[paraphrase_prediction]))\n",
    "print('BERT says that \"{}\" and \"{}\" are {}'.format(sequence_0, sequence_1, classes[not_paraphrase_prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the model in parallel on four cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_with_padding(batch, batch_size, max_length):\n",
    "    ## Reformulate the batch into three batch tensors - default batch size batches the outer dimension\n",
    "    encoded = batch['encoded']\n",
    "    inputs = torch.squeeze(encoded['input_ids'], 1)\n",
    "    attention = torch.squeeze(encoded['attention_mask'], 1)\n",
    "    token_type = torch.squeeze(encoded['token_type_ids'], 1)\n",
    "    quality = list(map(int, batch['quality']))\n",
    "\n",
    "    if inputs.size()[0] != batch_size:\n",
    "        print(\"Input size = {} - padding\".format(inputs.size()))\n",
    "        remainder = batch_size - inputs.size()[0]\n",
    "        zeros = torch.zeros( [remainder, max_length], dtype=torch.long )\n",
    "        inputs = torch.cat( [inputs, zeros] )\n",
    "        attention = torch.cat( [attention, zeros] )\n",
    "        token_type = torch.cat( [token_type, zeros] )\n",
    "\n",
    "    assert(inputs.size()[0] == batch_size and inputs.size()[1] == max_length)\n",
    "    assert(attention.size()[0] == batch_size and attention.size()[1] == max_length)\n",
    "    assert(token_type.size()[0] == batch_size and token_type.size()[1] == max_length)\n",
    "\n",
    "    return (inputs, attention, token_type), quality\n",
    "\n",
    "def count(output, quality):\n",
    "    assert output.size(0) >= len(quality)\n",
    "    correct_count = 0\n",
    "    count = len(quality)\n",
    "    \n",
    "    batch_predictions = [ row.argmax().item() for row in output ]\n",
    "\n",
    "    for a, b in zip(batch_predictions, quality):\n",
    "        if int(a)==int(b):\n",
    "            correct_count += 1\n",
    "\n",
    "    return correct_count, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Histogram throughput (UTC times):\n",
      "\n",
      "===\n",
      "\n",
      "23:03:42.008 - 23:03:42.183 => 229 sentences/sec\n",
      "\n",
      "23:03:42.183 - 23:03:42.357 => 229 sentences/sec\n",
      "\n",
      "23:03:42.357 - 23:03:42.532 => 206 sentences/sec\n",
      "\n",
      "23:03:42.532 - 23:03:42.707 => 229 sentences/sec\n",
      "\n",
      "23:03:42.707 - 23:03:42.881 => 206 sentences/sec\n",
      "\n",
      "23:03:42.881 - 23:03:43.056 => 229 sentences/sec\n",
      "\n",
      "23:03:43.056 - 23:03:43.231 => 206 sentences/sec\n",
      "\n",
      "23:03:43.231 - 23:03:43.405 => 206 sentences/sec\n",
      "\n",
      "23:03:43.405 - 23:03:43.580 => 229 sentences/sec\n",
      "\n",
      "23:03:43.580 - 23:03:43.754 => 206 sentences/sec\n",
      "\n",
      "23:03:43.754 - 23:03:43.929 => 160 sentences/sec\n",
      "\n",
      "23:03:43.929 - 23:03:44.104 => 137 sentences/sec\n",
      "\n",
      "23:03:44.104 - 23:03:44.278 => 206 sentences/sec\n",
      "\n",
      "23:03:44.278 - 23:03:44.453 => 206 sentences/sec\n",
      "\n",
      "23:03:44.453 - 23:03:44.628 => 206 sentences/sec\n",
      "\n",
      "23:03:44.628 - 23:03:44.802 => 183 sentences/sec\n",
      "\n",
      "23:03:44.802 - 23:03:44.977 => 206 sentences/sec\n",
      "\n",
      "23:03:44.977 - 23:03:45.152 => 206 sentences/sec\n",
      "\n",
      "23:03:45.152 - 23:03:45.326 => 206 sentences/sec\n",
      "\n",
      "23:03:45.326 - 23:03:45.501 => 183 sentences/sec\n",
      "\n",
      "23:03:45.501 - 23:03:45.676 => 206 sentences/sec\n",
      "\n",
      "23:03:45.676 - 23:03:45.850 => 206 sentences/sec\n",
      "\n",
      "23:03:45.850 - 23:03:46.025 => 183 sentences/sec\n",
      "\n",
      "23:03:46.025 - 23:03:46.200 => 68 sentences/sec\n",
      "\n",
      "23:03:46.200 - 23:03:46.374 => 206 sentences/sec\n",
      "\n",
      "23:03:46.374 - 23:03:46.549 => 183 sentences/sec\n",
      "\n",
      "23:03:46.549 - 23:03:46.723 => 206 sentences/sec\n",
      "\n",
      "23:03:46.723 - 23:03:46.898 => 206 sentences/sec\n",
      "\n",
      "23:03:46.898 - 23:03:47.073 => 206 sentences/sec\n",
      "\n",
      "23:03:47.073 - 23:03:47.247 => 183 sentences/sec\n",
      "\n",
      "23:03:47.247 - 23:03:47.422 => 206 sentences/sec\n",
      "\n",
      "23:03:47.422 - 23:03:47.597 => 206 sentences/sec\n",
      "\n",
      "23:03:47.597 - 23:03:47.771 => 206 sentences/sec\n",
      "\n",
      "23:03:47.771 - 23:03:47.946 => 183 sentences/sec\n",
      "\n",
      "23:03:47.946 - 23:03:48.121 => 206 sentences/sec\n",
      "\n",
      "23:03:48.121 - 23:03:48.295 => 91 sentences/sec\n",
      "\n",
      "23:03:48.295 - 23:03:48.470 => 206 sentences/sec\n",
      "\n",
      "23:03:48.470 - 23:03:48.645 => 183 sentences/sec\n",
      "\n",
      "23:03:48.645 - 23:03:48.819 => 206 sentences/sec\n",
      "\n",
      "23:03:48.819 - 23:03:48.994 => 206 sentences/sec\n",
      "\n",
      "23:03:48.994 - 23:03:49.169 => 206 sentences/sec\n",
      "\n",
      "23:03:49.169 - 23:03:49.343 => 183 sentences/sec\n",
      "\n",
      "23:03:49.343 - 23:03:49.518 => 206 sentences/sec\n",
      "\n",
      "23:03:49.518 - 23:03:49.692 => 206 sentences/sec\n",
      "\n",
      "23:03:49.692 - 23:03:49.867 => 183 sentences/sec\n",
      "\n",
      "23:03:49.867 - 23:03:50.042 => 206 sentences/sec\n",
      "\n",
      "23:03:50.042 - 23:03:50.216 => 206 sentences/sec\n",
      "\n",
      "23:03:50.216 - 23:03:50.391 => 114 sentences/sec\n",
      "\n",
      "23:03:50.391 - 23:03:50.566 => 114 sentences/sec\n",
      "\n",
      "23:03:50.566 - 23:03:50.740 => 206 sentences/sec\n",
      "\n",
      "23:03:50.740 - 23:03:50.915 => 206 sentences/sec\n",
      "\n",
      "23:03:50.915 - 23:03:51.090 => 206 sentences/sec\n",
      "\n",
      "23:03:51.090 - 23:03:51.264 => 206 sentences/sec\n",
      "\n",
      "23:03:51.264 - 23:03:51.439 => 183 sentences/sec\n",
      "\n",
      "23:03:51.439 - 23:03:51.614 => 206 sentences/sec\n",
      "\n",
      "23:03:51.614 - 23:03:51.788 => 206 sentences/sec\n",
      "\n",
      "23:03:51.788 - 23:03:51.963 => 183 sentences/sec\n",
      "\n",
      "23:03:51.963 - 23:03:52.137 => 206 sentences/sec\n",
      "\n",
      "23:03:52.137 - 23:03:52.312 => 206 sentences/sec\n",
      "\n",
      "23:03:52.312 - 23:03:52.487 => 206 sentences/sec\n",
      "\n",
      "\n",
      "\n",
      "Maximum throughput (histogram) = 229 sentences/sec\n",
      "\n",
      "Overall throughput (aggregate stats * parallel) = 217 sentences/sec\n",
      "\n",
      "\n",
      "\n",
      "Latency Percentiles:\n",
      "\n",
      "===\n",
      "\n",
      "P50  = 18 milliseconds\n",
      "\n",
      "P90  = 18 milliseconds\n",
      "\n",
      "P95  = 18 milliseconds\n",
      "\n",
      "P99  = 20 milliseconds\n",
      "\n",
      "P100 = 33 milliseconds\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:\n",
      "\n",
      "===\n",
      "\n",
      "Accuracy = 84.56% \n",
      "\n",
      "\n",
      "\n",
      "Sanity test:\n",
      "\n",
      "===\n",
      "\n",
      "Processed - num batches 510\n",
      "\n",
      "          - batch size 1\n",
      "\n",
      "          - num cores 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from parallel import NeuronSimpleDataParallel\n",
    "from bert_benchmark_utils import BertTestDataset, BertResults\n",
    "import time\n",
    "\n",
    "max_length = 128\n",
    "num_cores = 4\n",
    "batch_size = 1\n",
    "\n",
    "tsv_file=\"glue_mrpc_dev.tsv\"\n",
    "\n",
    "data_set = BertTestDataset( tsv_file=tsv_file, tokenizer=tokenizer, max_length=max_length )\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size*num_cores, shuffle=True, num_workers=2)\n",
    "\n",
    "# Create a model that will run parallel inferences on each core (code in parallel.py)\n",
    "parallel_neuron_model = NeuronSimpleDataParallel('bert_neuron.pt', num_cores)\n",
    "\n",
    "# Warm all cores\n",
    "z = torch.zeros( [num_cores * batch_size, max_length], dtype=torch.long )\n",
    "batch = (z, z, z)\n",
    "parallel_neuron_model(*batch)\n",
    "\n",
    "# Result aggregation class (code in bert_benchmark_utils.py)\n",
    "results = BertResults(batch_size, num_cores)\n",
    "\n",
    "for _ in range(5):\n",
    "    for batch in data_loader:\n",
    "        batch, quality = get_input_with_padding(batch, batch_size * num_cores, max_length)\n",
    "\n",
    "        start = time.time()\n",
    "        output = parallel_neuron_model(*batch)\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "\n",
    "        correct_count, inference_count = count(output, quality)\n",
    "        results.add_result( correct_count, inference_count, [elapsed], [end], elapsed )\n",
    "\n",
    "with open(\"benchmark.txt\", \"w\") as f:\n",
    "    results.report(f, bins=60)\n",
    "\n",
    "with open(\"benchmark.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recompile with a larger batch size of six sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:There are 3 ops of 1 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, (For more information see https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/neuron-cc-ops/neuron-cc-ops-pytorch.md)\n",
      "INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 714, fused = 694, percent fused = 97.2%\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1324 with neuron-cc\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/bin/neuron-cc compile /tmp/tmp9slz5gra/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /tmp/tmp9slz5gra/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[6, 128, 768], \"float32\"], \"1:0\": [[6, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Add_136:0\"]} --verbose 35'\n",
      "INFO:Neuron:Successfully embedded GraphDef into MetaNeff for _NeuronGraph#62\n",
      "INFO:Neuron:Number of arithmetic operators (post-compilation) before = 714, compiled = 694, percent compiled = 97.2%\n",
      "INFO:Neuron:The neuron partitioner created 1 sub-graphs\n",
      "INFO:Neuron:Neuron successfully compiled 1 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 100.0%\n",
      "INFO:Neuron:Compiled these operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 96\n",
      "INFO:Neuron: => aten::add: 108\n",
      "INFO:Neuron: => aten::addmm: 2\n",
      "INFO:Neuron: => aten::contiguous: 12\n",
      "INFO:Neuron: => aten::div: 12\n",
      "INFO:Neuron: => aten::dropout: 38\n",
      "INFO:Neuron: => aten::gelu: 12\n",
      "INFO:Neuron: => aten::layer_norm: 25\n",
      "INFO:Neuron: => aten::matmul: 96\n",
      "INFO:Neuron: => aten::permute: 48\n",
      "INFO:Neuron: => aten::select: 1\n",
      "INFO:Neuron: => aten::size: 96\n",
      "INFO:Neuron: => aten::slice: 1\n",
      "INFO:Neuron: => aten::softmax: 12\n",
      "INFO:Neuron: => aten::t: 74\n",
      "INFO:Neuron: => aten::tanh: 1\n",
      "INFO:Neuron: => aten::transpose: 12\n",
      "INFO:Neuron: => aten::view: 48\n",
      "INFO:Neuron:Not compiled operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 2 [supported]\n",
      "INFO:Neuron: => aten::ScalarImplicit: 1 [supported]\n",
      "INFO:Neuron: => aten::add: 2 [supported]\n",
      "INFO:Neuron: => aten::arange: 1 [supported]\n",
      "INFO:Neuron: => aten::embedding: 3 [not supported]\n",
      "INFO:Neuron: => aten::expand: 1 [supported]\n",
      "INFO:Neuron: => aten::mul: 1 [supported]\n",
      "INFO:Neuron: => aten::rsub: 1 [supported]\n",
      "INFO:Neuron: => aten::size: 2 [supported]\n",
      "INFO:Neuron: => aten::slice: 2 [supported]\n",
      "INFO:Neuron: => aten::to: 1 [supported]\n",
      "INFO:Neuron: => aten::unsqueeze: 3 [supported]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "\n",
    "example_inputs_paraphrase = (\n",
    "    torch.cat([paraphrase['input_ids']] * batch_size,0), \n",
    "    torch.cat([paraphrase['attention_mask']] * batch_size,0), \n",
    "    torch.cat([paraphrase['token_type_ids']] * batch_size,0)\n",
    ")\n",
    "\n",
    "# Run torch.neuron.trace to generate a TorchScript that is optimized by AWS Neuron\n",
    "model_neuron_batch = torch.neuron.trace(model, example_inputs_paraphrase)\n",
    "\n",
    "## Save the batched model\n",
    "model_neuron_batch.save('bert_neuron_b{}.pt'.format(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun inference with batch 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Histogram throughput (UTC times):\n",
      "\n",
      "===\n",
      "\n",
      "23:30:26.213 - 23:30:26.325 => 860 sentences/sec\n",
      "\n",
      "23:30:26.325 - 23:30:26.437 => 860 sentences/sec\n",
      "\n",
      "23:30:26.437 - 23:30:26.548 => 860 sentences/sec\n",
      "\n",
      "23:30:26.548 - 23:30:26.660 => 860 sentences/sec\n",
      "\n",
      "23:30:26.660 - 23:30:26.771 => 215 sentences/sec\n",
      "\n",
      "23:30:26.771 - 23:30:26.883 => 0 sentences/sec\n",
      "\n",
      "23:30:26.883 - 23:30:26.994 => 860 sentences/sec\n",
      "\n",
      "23:30:26.994 - 23:30:27.106 => 860 sentences/sec\n",
      "\n",
      "23:30:27.106 - 23:30:27.217 => 860 sentences/sec\n",
      "\n",
      "23:30:27.217 - 23:30:27.329 => 860 sentences/sec\n",
      "\n",
      "23:30:27.329 - 23:30:27.440 => 215 sentences/sec\n",
      "\n",
      "23:30:27.440 - 23:30:27.552 => 0 sentences/sec\n",
      "\n",
      "23:30:27.552 - 23:30:27.663 => 645 sentences/sec\n",
      "\n",
      "23:30:27.663 - 23:30:27.775 => 860 sentences/sec\n",
      "\n",
      "23:30:27.775 - 23:30:27.886 => 860 sentences/sec\n",
      "\n",
      "23:30:27.886 - 23:30:27.998 => 860 sentences/sec\n",
      "\n",
      "23:30:27.998 - 23:30:28.109 => 430 sentences/sec\n",
      "\n",
      "23:30:28.109 - 23:30:28.221 => 0 sentences/sec\n",
      "\n",
      "23:30:28.221 - 23:30:28.332 => 430 sentences/sec\n",
      "\n",
      "23:30:28.332 - 23:30:28.444 => 860 sentences/sec\n",
      "\n",
      "23:30:28.444 - 23:30:28.555 => 1076 sentences/sec\n",
      "\n",
      "23:30:28.555 - 23:30:28.667 => 860 sentences/sec\n",
      "\n",
      "23:30:28.667 - 23:30:28.778 => 430 sentences/sec\n",
      "\n",
      "23:30:28.778 - 23:30:28.890 => 0 sentences/sec\n",
      "\n",
      "23:30:28.890 - 23:30:29.001 => 430 sentences/sec\n",
      "\n",
      "23:30:29.001 - 23:30:29.113 => 860 sentences/sec\n",
      "\n",
      "23:30:29.113 - 23:30:29.224 => 860 sentences/sec\n",
      "\n",
      "23:30:29.224 - 23:30:29.336 => 860 sentences/sec\n",
      "\n",
      "23:30:29.336 - 23:30:29.447 => 645 sentences/sec\n",
      "\n",
      "23:30:29.447 - 23:30:29.559 => 0 sentences/sec\n",
      "\n",
      "23:30:29.559 - 23:30:29.670 => 215 sentences/sec\n",
      "\n",
      "23:30:29.670 - 23:30:29.782 => 860 sentences/sec\n",
      "\n",
      "23:30:29.782 - 23:30:29.893 => 860 sentences/sec\n",
      "\n",
      "23:30:29.893 - 23:30:30.005 => 860 sentences/sec\n",
      "\n",
      "23:30:30.005 - 23:30:30.116 => 860 sentences/sec\n",
      "\n",
      "23:30:30.116 - 23:30:30.228 => 0 sentences/sec\n",
      "\n",
      "23:30:30.228 - 23:30:30.339 => 0 sentences/sec\n",
      "\n",
      "23:30:30.339 - 23:30:30.451 => 860 sentences/sec\n",
      "\n",
      "23:30:30.451 - 23:30:30.562 => 860 sentences/sec\n",
      "\n",
      "23:30:30.562 - 23:30:30.674 => 860 sentences/sec\n",
      "\n",
      "23:30:30.674 - 23:30:30.785 => 860 sentences/sec\n",
      "\n",
      "23:30:30.785 - 23:30:30.897 => 215 sentences/sec\n",
      "\n",
      "23:30:30.897 - 23:30:31.008 => 0 sentences/sec\n",
      "\n",
      "23:30:31.008 - 23:30:31.120 => 645 sentences/sec\n",
      "\n",
      "23:30:31.120 - 23:30:31.231 => 860 sentences/sec\n",
      "\n",
      "23:30:31.231 - 23:30:31.343 => 860 sentences/sec\n",
      "\n",
      "23:30:31.343 - 23:30:31.454 => 860 sentences/sec\n",
      "\n",
      "23:30:31.454 - 23:30:31.566 => 430 sentences/sec\n",
      "\n",
      "23:30:31.566 - 23:30:31.677 => 0 sentences/sec\n",
      "\n",
      "23:30:31.677 - 23:30:31.789 => 430 sentences/sec\n",
      "\n",
      "23:30:31.789 - 23:30:31.900 => 860 sentences/sec\n",
      "\n",
      "23:30:31.900 - 23:30:32.012 => 645 sentences/sec\n",
      "\n",
      "23:30:32.012 - 23:30:32.123 => 860 sentences/sec\n",
      "\n",
      "23:30:32.123 - 23:30:32.235 => 860 sentences/sec\n",
      "\n",
      "23:30:32.235 - 23:30:32.346 => 0 sentences/sec\n",
      "\n",
      "23:30:32.346 - 23:30:32.458 => 215 sentences/sec\n",
      "\n",
      "23:30:32.458 - 23:30:32.569 => 645 sentences/sec\n",
      "\n",
      "23:30:32.569 - 23:30:32.681 => 860 sentences/sec\n",
      "\n",
      "23:30:32.681 - 23:30:32.792 => 860 sentences/sec\n",
      "\n",
      "23:30:32.792 - 23:30:32.904 => 1076 sentences/sec\n",
      "\n",
      "\n",
      "\n",
      "Maximum throughput (histogram) = 1076 sentences/sec\n",
      "\n",
      "Overall throughput (aggregate stats * parallel) = 809 sentences/sec\n",
      "\n",
      "\n",
      "\n",
      "Latency Percentiles:\n",
      "\n",
      "===\n",
      "\n",
      "P50  = 25 milliseconds\n",
      "\n",
      "P90  = 34 milliseconds\n",
      "\n",
      "P95  = 58 milliseconds\n",
      "\n",
      "P99  = 81 milliseconds\n",
      "\n",
      "P100 = 83 milliseconds\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:\n",
      "\n",
      "===\n",
      "\n",
      "Accuracy = 84.39% \n",
      "\n",
      "\n",
      "\n",
      "Sanity test:\n",
      "\n",
      "===\n",
      "\n",
      "Processed - num batches 170\n",
      "\n",
      "          - batch size 6\n",
      "\n",
      "          - num cores 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from parallel import NeuronSimpleDataParallel\n",
    "from bert_benchmark_utils import BertTestDataset, BertResults\n",
    "import time\n",
    "\n",
    "batch_size = 6\n",
    "num_cores = 4\n",
    "\n",
    "tsv_file=\"glue_mrpc_dev.tsv\"\n",
    "\n",
    "data_set = BertTestDataset( tsv_file=tsv_file, tokenizer=tokenizer, max_length=max_length )\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size*num_cores, shuffle=True, num_workers=2)\n",
    "\n",
    "# Create a model that will run parallel inferences on each core (code in parallel.py)\n",
    "parallel_neuron_model = NeuronSimpleDataParallel('bert_neuron_b{}.pt'.format(batch_size), num_cores, batch_size)\n",
    "\n",
    "# Warm all cores\n",
    "z = torch.zeros( [num_cores * batch_size, max_length], dtype=torch.long )\n",
    "batch = (z, z, z)\n",
    "parallel_neuron_model(*batch)\n",
    "\n",
    "# Result aggregation class (code in bert_benchmark_utils.py)\n",
    "results = BertResults(batch_size, num_cores)\n",
    "\n",
    "for _ in range(10):\n",
    "    for batch in data_loader:\n",
    "        batch, quality = get_input_with_padding(batch, batch_size * num_cores, max_length)\n",
    "\n",
    "        start = time.time()\n",
    "        output = parallel_neuron_model(*batch)\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "\n",
    "        correct_count, inference_count = count(output, quality)\n",
    "        results.add_result( correct_count, inference_count, [elapsed], [end], elapsed )\n",
    "\n",
    "with open(\"benchmark_b{}.txt\".format(batch_size), \"w\") as f:\n",
    "    results.report(f, bins=60)\n",
    "\n",
    "with open(\"benchmark_b{}.txt\".format(batch_size), \"r\") as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_aws_neuron_pytorch_p36)",
   "language": "python",
   "name": "conda_aws_neuron_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
