diff --git a/src/llmperf/ray_clients/openai_chat_completions_client.py b/src/llmperf/ray_clients/openai_chat_completions_client.py
index f2e0a91..644d5a6 100644
--- a/src/llmperf/ray_clients/openai_chat_completions_client.py
+++ b/src/llmperf/ray_clients/openai_chat_completions_client.py
@@ -92,7 +92,7 @@ class OpenAIChatCompletionsClient(LLMClient):
                     if delta.get("content", None):
                         if not ttft:
                             ttft = time.monotonic() - start_time
-                            time_to_next_token.append(ttft)
+                            # time_to_next_token.append(ttft)
                         else:
                             time_to_next_token.append(
                                 time.monotonic() - most_recent_received_token_time
diff --git a/token_benchmark_ray.py b/token_benchmark_ray.py
index 63216b1..11e0116 100644
--- a/token_benchmark_ray.py
+++ b/token_benchmark_ray.py
@@ -32,6 +32,7 @@ def get_token_throughput_latencies(
     stddev_input_tokens: int,
     mean_output_tokens: int,
     stddev_output_tokens: int,
+    tokenizer: str,
     additional_sampling_params: Optional[Dict[str, Any]] = None,
     num_concurrent_requests: int = 1,
     max_num_completed_requests: int = 500,
@@ -60,10 +61,8 @@ def get_token_throughput_latencies(
     """
     random.seed(11111)
 
-    tokenizer = LlamaTokenizerFast.from_pretrained(
-        "hf-internal-testing/llama-tokenizer"
-    )
-    get_token_length = lambda text: len(tokenizer.encode(text))
+    hf_tokenizer = LlamaTokenizerFast.from_pretrained(tokenizer)
+    get_token_length = lambda text: len(hf_tokenizer.encode(text))
     
     if not additional_sampling_params:
         additional_sampling_params = {}
@@ -84,7 +83,7 @@ def get_token_throughput_latencies(
             prompt_tokens_mean=mean_input_tokens,
             prompt_tokens_stddev=stddev_input_tokens,
             expect_output_tokens=num_output_tokens,
-            tokenizer=tokenizer
+            tokenizer=hf_tokenizer
         ))
     start_time = time.monotonic()
     pbar = tqdm(total=max_num_completed_requests)
@@ -118,7 +117,7 @@ def get_token_throughput_latencies(
                 with completed_requests_lock:
                     if num_completed_requests < max_num_completed_requests:
                         if num_output_tokens:
-                            request_metrics[common_metrics.INTER_TOKEN_LAT] /= request_metrics[common_metrics.NUM_OUTPUT_TOKENS]
+                            request_metrics[common_metrics.INTER_TOKEN_LAT] /= num_output_tokens - 1
                         else:
                             request_metrics[common_metrics.INTER_TOKEN_LAT] = 0
                         request_metrics[common_metrics.NUM_OUTPUT_TOKENS] = num_output_tokens
@@ -155,7 +154,7 @@ def get_token_throughput_latencies(
         with completed_requests_lock:
             if num_completed_requests < max_num_completed_requests:
                 if num_output_tokens:
-                    request_metrics[common_metrics.INTER_TOKEN_LAT] /= num_output_tokens
+                    request_metrics[common_metrics.INTER_TOKEN_LAT] /= num_output_tokens - 1
                 else:
                     request_metrics[common_metrics.INTER_TOKEN_LAT] = 0
                 request_metrics[common_metrics.NUM_OUTPUT_TOKENS] = num_output_tokens
@@ -292,6 +291,7 @@ def run_token_benchmark(
     additional_sampling_params: str,
     results_dir: str,
     user_metadata: Dict[str, Any],
+    tokenizer: str,
 ):
     """
     Args:
@@ -327,6 +327,7 @@ def run_token_benchmark(
         stddev_output_tokens=stddev_output_tokens,
         num_concurrent_requests=num_concurrent_requests,
         additional_sampling_params=json.loads(additional_sampling_params),
+        tokenizer=tokenizer,
     )
 
     if results_dir:
@@ -462,6 +463,11 @@ args.add_argument(
         "name=foo,bar=1. These will be added to the metadata field of the results. "
     ),
 )
+args.add_argument(
+    "--tokenizer",
+    type=str,
+    default="hf-internal-testing/llama-tokenizer",
+)
 
 if __name__ == "__main__":
     env_vars = dict(os.environ)
@@ -488,4 +494,5 @@ if __name__ == "__main__":
         additional_sampling_params=args.additional_sampling_params,
         results_dir=args.results_dir,
         user_metadata=user_metadata,
+        tokenizer=args.tokenizer,
     )
